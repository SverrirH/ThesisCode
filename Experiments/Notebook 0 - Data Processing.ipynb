{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not in this notebook\n",
    "- computing the CAPPI layer (Takes too long)\n",
    "\n",
    "\n",
    "# In this notebook\n",
    "- Processing all data types and leaving the nan values so they can be removed at the latest possible moment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "from dataProcessingHelpers import *\n",
    "from missingno import matrix\n",
    "\n",
    "min_date = datetime.datetime(2015,1,1)\n",
    "URL_save_data = 'C:/Users/sverrirhd/Google Drive/Skóli/DTU/Thesis/Programming/Experiments/Data/'\n",
    "\n",
    "URL_raw_temperature_data = 'C:/Users/sverrirhd/Google Drive/Skóli/DTU/Thesis/Data/Temperature data/hbs_hitastig_klst.csv'\n",
    "\n",
    "\n",
    "URL_processed_temperature_data = URL_save_data + 'temperature_data.pkl'\n",
    "URL_processed_temperature_forecast = URL_save_data + 'temperature_forecast_data.pkl'\n",
    "URL_processed_drainage_flow = URL_save_data + 'drainage_flow.pkl'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature data\n",
    "- Read raw data\n",
    "- Remove bad values\n",
    "- Pivot table\n",
    "- save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_long = pd.read_csv(URL_raw_temperature_data,sep='\\t')\n",
    "df_temp_long.loc[:,'TIMI'] = pd.to_datetime(df_temp_long.loc[:,'TIMI'])\n",
    "df_temp_long.loc[:,'T'] = df_temp_long.loc[:,'T'].replace({'(null)':np.nan}).astype(float)\n",
    "df_temp_long.STOD = df_temp_long.STOD.astype('str')\n",
    "df_temp = df_temp_long.pivot_table(values='T',index='TIMI',columns='STOD')\n",
    "df_temp = df_temp.loc[min_date:].astype(float)\n",
    "df_temp.to_pickle(URL_processed_temperature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature forecast\n",
    "- Requires a VPN connection with Veitur\n",
    "- Queries all historical forecasts for weather station 1475\n",
    "- Processes data:\n",
    "    - Datetime conversions\n",
    "    - Computes the offset of the prediction vs the predicted date\n",
    "    - Pivots the table so that each row is 'now' and each consecutive column represents +1 hour into the future\n",
    "    - Resamples data to hourly intervals and replaces missing data with np.nan\n",
    "    - Interpolates backwards (i.e. won't try to extrapolate beyond last available forecast date)\n",
    "    - Fill in missing spaces with best available forecast. (i.e. newest forecast that has predicted for that period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_temp_forecast =\\\n",
    "''' SELECT date_time,value_date,temperature\n",
    "    FROM [DM_Orkuveitan].[dbo].[fact_vedurspar]\n",
    "    WHERE station_id = 1475 \n",
    "    AND value_date >= \\'2014-12-20\\' \n",
    "    AND value_date <= \\'2021-01-01\\'\n",
    "'''\n",
    "df_spa_raw = execute_query(connect_to_db(),query_temp_forecast)\n",
    "df_spa = process_temperature_predictions(df_spa_raw,min_date).astype(float)\n",
    "df_spa.to_pickle(URL_processed_temperature_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drainage data\n",
    "- Open metadata file\n",
    "- Get list of sensor names to use\n",
    "- Load each batch of sensor dataframes into a list \n",
    "- Use only average values (ctype = 4) with a good flag (flag = 0)\n",
    "- Pivot all tables into a single dataframe with a common datetime index\n",
    "- If any value is less than 0 or equal to 0, 1 or 2 precisely, then they are errors, so we remove those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(URL_save_data + 'drainage_meta_data.json', encoding='utf-8')\n",
    "df = pd.DataFrame(json.load(f)['sensors'])\n",
    "\n",
    "get_flow = lambda x : sensor_data_query(x['ID'], x['flow'],earliest_date = min_date)\n",
    "df.loc[:,'flow_query'] = df.apply(get_flow,axis=1)\n",
    "\n",
    "conn = connect_to_db()\n",
    "list_flow = []\n",
    "for index,row in df.iterrows():\n",
    "    data_flow_raw = execute_query(conn,row['flow_query'],index_col = 'CTime')\n",
    "    data_flow = process_drainage_sensors(data_flow_raw)\n",
    "    data_flow.name = row['ID']\n",
    "    list_flow.append(data_flow)\n",
    "    \n",
    "df_drainage_flow = pd.concat(list_flow,axis=1).astype(float)\n",
    "df_drainage_flow.to_pickle(URL_processed_drainage_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".reshape(len(df_X_NWP),-1,10,10)[:,:pred_dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37b26b0919f3eebb53e1fb9d1c196dfd7351d15ccc4a50e8438da0372db545ef"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('gis_wradlib_torch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
